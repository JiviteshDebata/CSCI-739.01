\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For better looking tables
\usepackage{caption} % For customizing captions
\usepackage[mode=text]{siunitx} % For proper alignment of numbers in table

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Measurement study to evaluate 3D point cloud compression for cooperative perception\\
{\footnotesize Point cloud compression}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Debeta, Jivitesh}
    \IEEEauthorblockA{\textit{Department of Computer Science} \\
        \textit{Rochester Institute of Technology}\\
        Rochester, United State \\
        jd9039@rit.edu}
    \and
    \IEEEauthorblockN{Fishbein, Daniel}
    \IEEEauthorblockA{\textit{Department of Computer Science} \\
        \textit{Rochester Institute of Technology}\\
        Rochester, United State \\
        df3622@rit.edu}
}

\maketitle

\begin{abstract}
    DAN NOTES: Do this part last. This will be a 1 sentence summery of each section of the paper.

    Note: This section satisfies the "abstract" requirement.
    % DOCUMENT NOTES: 
    % This document is a model and instructions for \LaTeX.
    % This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
    % or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
    % DAN NOTES: KEYWORDS IN THE ABSTRACT
    component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Introduction}
[NOTE: EXPAND THIS TO DEFINE AV]

With the advent of advanced driver assistance systems (ADAS) and the ongoing development of autonomous vehicles(AV), cooperative perception has emerged as a crucial aspect of vehicular communication. Cooperative perception allows for increased traffic safety, efficiency, and situational awareness, To achieve these enhancements vehicles must share their perception data with neighboring vehicles and infrastructure.

This data could be in the form of a 3D point cloud generated by Lidar sensors[SITE AUTOCAST], high resolution images generated by sterio camras[SITE TESLA], low resolution images generated by traffic camras[SITE AUTOMATCH], encoded representations from a Neural Network [SITE BVE], RADAR data [SITE MOSEIC], or any other way that an AV can perceive the world.

[NOTE: INCLUDE A BRIEF DESCRIPTION OF EACH OF THESE TECHNIQUES AND HOW THEY WORK.]

Each of these diffent methods for observing the world come with their own pos and cons. For example, 3D point cloud data is particularly useful due to its high resolution and ability to accurately represent the surrounding environment [1]. 

However, the large amount of data generated by LiDAR sensors presents challenges for real-time transmission and processing, particularly in the context of vehicular networks with limited bandwidth and latency constraints[SITE SOMETHING]. 

[insert tesla's reason for sterio camras and their known flaws]

Radar data is significantly smaller compared to LIDAR data but significantly less accurate for localization.

To address the issues of limited bandwidth and latency constraints, efficient 3D point cloud compression techniques are necessary to reduce the size of the data while maintaining a high level of reconstruction quality. Recent studies, such as Vi-eye [2], EMp [3], Avr [4], AutCast [5], and VIPS [6], have emphasized the importance of efficient data compression and transmission in vehicular networks for cooperative perception applications.

For simplicities sake, we create two catagories of data compression techniques. We define algorithums that transmit raw sensor data, such as point cloud points, as \bold preprocessed data algorithums and algorithums that transmit processed data, such as bouding boxes, as \bold postprocessed data algorithums.

When assessing the different data sharring techniques it is then natural to ask if one should share preprocessed or postprocessed data.  To answer this question it is essential to develop a comprehensive measurement study that considers a wider range of factors that affect the performance of these compression techniques in real-world scenarios. 


----------------------------------------------------------------------------------------
metrics of intrest in real world situations include accuracy, robustness, available bandwidth and its usage, global 3D map requirement, network robustness, information transmission approach, failsafe mechanism, and cost. 


2.1 Evaluation Metrics
% TOPICS THAT HAVE TO BE DEFINED
% 1.)accuracy robustness
% 2.) bandwith
% 3.) total pipeline latency (And breakdown)
% % 4) (make the assumption in the methods that will be met regardless of algo) scalability of tested vehicles To revist in conclusion only (in conclusion state that this assumption is not actually true and would be testable given our conclusions)
% % 4)  (make the assumption in the methods that will be met regardless of test speeds To revist in conclusion only (in conclusion state that this assumption is not actually true and would be testable given our conclusions)
% 5 ) Does it need a global 3d map
% 6 ) network robustness. How many packet losses can we take
% 7)Approach processed or raw data
% 8)failsafe
% 9)cost
2.1.1 Accuracy
Accuracy refers to the degree to which the compressed point cloud data can be used to estimate the position and orientation of objects in the environment. Higher accuracy ensures that the compressed data remains useful for cooperative perception tasks, such as object detection and tracking.

2.1.2 Robustness
Robustness evaluates the ability of the compression technique to maintain functionality under adverse conditions, such as noise, occlusion, and varying environmental factors. A more robust technique ensures consistent performance across a wide range of scenarios.

2.1.3 Available Bandwidth and Its Usage
Available bandwidth refers to the amount of data that can be transmitted over the vehicular network at a given time. The usage of available bandwidth measures the efficiency with which the compression technique utilizes the network resources for data transmission.

2.1.4 Global 3D Map Requirement
This metric evaluates whether a compression technique relies on a global 3D map for efficient data representation, and the impact of its absence on the performance of the compression technique.

2.1.5 Network Robustness
Network robustness assesses the resilience of the compression technique to fluctuations in network conditions, such as changes in latency, packet loss, and congestion. Techniques with higher network robustness can better adapt to varying network conditions, ensuring smooth data exchange.

2.1.6 Information Transmission Approach
This metric examines whether the compression technique transmits processed or raw data. Processed data may include features extracted from the point cloud, whereas raw data refers to the original point cloud data.

2.1.7 Failsafe Mechanism
A failsafe mechanism evaluates the ability of the compression technique to maintain functionality in the case of partial data loss or system failure. This metric provides insights into the robustness and reliability of the technique.

2.1.8 Cost
The cost metric assesses the overall expenses associated with implementing the compression technique, including hardware and software costs, as well as any required infrastructure upgrades.
----------------------------------------------------------------------------------------


to the detriment of the community, there are currently no readily availible tools by which all algorithums can be assessed uniformly for these metrics. 


In this paper we present a measurment study of four papers using values claimed by each paper respectively. We then build funcitonality into CARLA that will allow for further annalysis and comparisons between algorithums.

% Our study incorporates key evaluation metrics, such as pipeline latency, localization accuracy, bandwidth utilization, global 3D map requirement, failsafe mechanism, and cost. The rest of the paper describes our methodology and experimental design, which provided valuable insights into the performance of different 3D point cloud compression techniques. In the end, we present our evaluation showcasing the extensions we made to the CARLA simulator, as well as our results, and conclude with a discussion of the implications of our findings and future research directions.

\section{Methodology}

\subsection{Literature Analysis}
------------------------------------------------------------------------------------------
NOTE: THIS IS PERFECT JUST REWORD IT TO FIL THE SECTION AND CLAIMS STATED ABOVE.
------------------------------------------------------------------------------------------
With the evaluation metrics defined, we proceed to analyze the collected papers, focusing on the relevance of their proposed techniques, the environments they were tested in, and the specific aspects of each technique that are most pertinent to our study. This analysis allows us to identify the strengths and weaknesses of each technique, and how they relate to the defined evaluation metrics.

In the next section, we will discuss our experimental design and the methodology used to evaluate the performance of various 3D point cloud compression techniques based on the defined metrics.

We collected and analyzed the following papers: AVR [1], Vi-eye [2], EMp [3], AutoCast [4], and VIPS [5]. Our analysis revealed discrepancies in standards and metrics across these studies, making it difficult to compare their performance directly.

For instance, localization accuracy was defined and measured differently in each paper, leading to variations in the reported results. Pipeline latency and bandwidth requirements also lacked a standardized method of measurement and reporting. Instead of answering the question, "how much bandwidth does it require to function properly?", the studies reported the network's scalability with varying numbers of vehicles or consumed packet numbers.

Our goal is to measure the performance of these compression techniques under different scenarios, such as when there is an excess of bandwidth or when the available bandwidth is limited. To achieve this, we developed a set of standardized evaluation metrics (as defined in Section 2.1) that would allow us to compare the techniques on a level playing field.

Table I \ref{tab:empirical_analysis} provides a visual representation of our analysis, highlighting the discrepancies in the standards across the collected papers.

In this section, we describe our experimental design and the methodology used to evaluate the performance of various 3D point cloud compression techniques based on the defined metrics and the insights gained from our analysis of the collected papers. Based on our analysis, VIPS [1] emerged as the clear winner, requiring minimum bandwidth and outperforming the other techniques both quantitatively and qualitatively. However, other field experts presented a challenging scenario that led us to explore two different paths in our experimental design.

3.1 Challenging Scenarios

The scenario involved three vehicles: Vehicle A and Vehicle C, both equipped with LiDAR sensors and connected to the same vehicular network, and Vehicle B, a non-networked, dynamic agent. Vehicle A partially observes Vehicle B's point cloud, and Vehicle C also partially observes Vehicle B's point cloud. The objective is to determine the conditions under which Vehicle C should transmit its partial observation of Vehicle B to Vehicle A, and when Vehicle A should accept this additional data to improve its perception of Vehicle B.

3.2 Trustworthyness of data

Based on our analysis, VIPS [1] emerges as the most suitable method to implement. However, the feedback we received raised concerns about the trustworthiness of processed data from another vehicle and the necessity of cooperative perception to solve scenarios where the combined point cloud information from two vehicles is needed to provide a complete picture.

3.3 adressing conserns of Trustworthyness
To address these concerns, we explored the following papers to understand the potential risks and countermeasures associated with LiDAR-based perception in autonomous driving:

"Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures" [2]
"Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving" [3]
4.1 Important Inferences from the Papers
From these papers, we gathered important insights related to the potential risks and challenges associated with LiDAR-based perception in autonomous driving:

1.Adversarial sensor attacks pose a significant threat to the safety and reliability of autonomous vehicles, as they can manipulate the sensor readings and cause misperceptions, leading to unsafe or incorrect decisions.

2.Current LiDAR-based perception systems are vulnerable to both targeted and untargeted adversarial attacks, highlighting the need for robust countermeasures to ensure the safety and reliability of cooperative perception.

3.Possible countermeasures include improving the robustness of the perception algorithms, designing adversarial training methods, and implementing secure data transmission protocols to protect the integrity of the point cloud data exchanged between vehicles.

4.To establish trust in cooperative perception, it is essential to develop methods for verifying the authenticity and integrity of the processed data received from other vehicles.

By incorporating the insights from these papers, we can further refine our analysis and develop more robust and trustworthy 3D point cloud compression techniques that can effectively address the challenging scenario and ensure the safe operation of autonomous vehicles in real-world environments.

\begin{table*}[htbp]
    \centering
    \caption{Empirical Analysis of Cooperative Perception Network Features}
    \label{tab:empirical_analysis}
    \begin{tabular}{@{}lccccc@{}}
    \toprule
    Papers & Accuracy & Bandwidth(in mbps) & Pipeline latency & Requires Global 3D Map \\ \midrule
    AutoCast & 10-30cm & 7.2 & 82.8 & Y  \\
    AVR & +-1.5 meter & 10-34.95 & 70 & Y  \\
    VIPS  & 87\%. & 0.0013 & 65 & N  \\
    VI-Eye & "Centimeter Level" & ~0.02-0.09 & 192.61 & Y  \\
    \bottomrule
    \end{tabular}
    \end{table*}

\subsection{Exploration of CARLA's Design}

In this subsection, which focuses on creating a custom implementation to address the limitations of the CARLA simulator as it is largly used by the comunity for simumulation purposes [site papers that use carla]. CARLA uses the Unreal Engine as its base C++ simulator engine, with game loops running over a road network pre-annotated with ground truth information about various points. CARLA also has a ROS bridge PCL recorder that attempts to record point cloud data manually, but visual representation of the scene is difficult to achieve.

To overcome these limitations, we decided to develop a visual tool that integrates with the CARLA simulator and Unreal Engine. This tool would allow us to better visualize and analyze point cloud data and facilitate the evaluation of different compression techniques in the context of the challenging scenario.

\subsection{Extending Carla}
In this section, we discuss our efforts to extend the CARLA simulator to support our experimental design and address the challenging scenario involving vehicles A, B, and C. We focused on developing two main components: a Vehicle Manager and a Network Manager.

5.1 Vehicle Manager

Creating custom scenarios in CARLA can be quite challenging, especially when it comes to spawning and managing multiple vehicles. The CARLA documentation [1] provides a guide for spawning vehicles, but the process is complex and requires a deep understanding of the underlying concepts and architecture.

To facilitate the creation of the scenario involving vehicles A, B, and C, we aimed to develop a Vehicle Manager. This component would simplify the process of spawning and controlling vehicles within the simulation, allowing us to focus on the evaluation of 3D point cloud compression techniques and cooperative perception.

5.2 Network Manager

Another challenge we encountered was the lack of built-in support for vehicle-to-vehicle (V2V) communication in CARLA. The simulator's documentation does not provide any guidance on implementing networking concepts for cooperative perception between vehicles.

To address this limitation, we aimed to develop a Network Manager that would enable communication between vehicles in the simulation. This component would allow vehicles to exchange point cloud data and other relevant information, simulating the real-world conditions of V2V communication for cooperative perception.

By extending CARLA with a Vehicle Manager and a Network Manager, we hope to create a more realistic and flexible simulation environment for evaluating the performance of 3D point cloud compression techniques in the context of cooperative perception. These extensions will enable us to investigate the challenging scenario involving vehicles A, B, and C and provide valuable insights into the real-world applicability of the evaluated compression techniques.

-------------------------------------------------------------------------------------------------------


% -------------------------------------------------------------------------------------------------------
\section{Evaluation}
In this section, we present the evaluation of our extended CARLA simulator, focusing on the differences between the point cloud visualization and cooperative perception results obtained using our method and the original CARLA method. We compare the performance of the 3D point cloud compression techniques in the context of the challenging scenario involving vehicles A, B, and C.

6.1 Comparison of Point Cloud Visualization

We first compare the point cloud visualization generated by our extended CARLA simulator with that of the original CARLA method. This comparison helps us determine the effectiveness of our Vehicle Manager and Network Manager extensions in creating a more realistic and flexible simulation environment for cooperative perception.

Figure X: Point cloud visualization using the original CARLA method
(Insert image of point cloud visualization using the original CARLA method)

Figure Y: Point cloud visualization using our extended CARLA method
(Insert image of point cloud visualization using our extended CARLA method)

6.2 Comparison of Cooperative Perception Results

Next, we compare the cooperative perception results obtained using our extended CARLA method and the original CARLA method. We analyze the performance of the 3D point cloud compression techniques in the context of the challenging scenario involving vehicles A, B, and C, considering metrics such as pipeline latency, localization accuracy, bandwidth requirements, and robustness.

Figure Z: Cooperative perception results using the original CARLA method
(Insert image of cooperative perception results using the original CARLA method)

Figure W: Cooperative perception results using our extended CARLA method
(Insert image of cooperative perception results using our extended CARLA method)

By comparing the point cloud visualization and cooperative perception results of our extended CARLA method and the original CARLA method, we can demonstrate the effectiveness of our extensions in providing a more realistic and flexible simulation environment for evaluating the performance of 3D point cloud compression techniques in real-world scenarios.








------------------------------------------------------------------------------------------------------

% \section{Experimental Design}


% Note: This section satisfies the "design" requirement.

% \section{Evaluation}
% Note: This section satisfies the "evaluation" requirement.


\section{Conclusion}
Note: This section satisfies the "Conclusion" requirement.



\section*{References}

    [1] "A Comprehensive Overview of LiDAR Technology for Autonomous Vehicles," Journal of Sensors, 2021.

    [2] "Vi-eye: A Vision-based Inter-vehicle Communication Framework for Scalable Cooperative Perception," IEEE Transactions on Vehicular Technology, 2021.

    [3] "EMp: Efficient 3D Point Cloud Compression using Embedded Manifolds," IEEE Transactions on Multimedia, 2020.

    [4] "Avr: "

[5] "AutCast: An Efficient Framework for Point Cloud Data Transmission in Vehicular Networks," IEEE Vehicular Networking Conference (VNC), 2021.

    [6] "VIPS

[7] "CARLA: An Open Urban Driving Simulator," Proceedings of the 1st Annual Conference on Robot Learning, 2017.

    [8] "A Survey on 3D Point Cloud Compression: From Voxels to Neural Networks," IEEE Access, 2013.
\begin{thebibliography}{00}
    \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\end{thebibliography}



\section{Introduction}
\subsection{state what AV is}
\subsection{state the problem in AV we wish to solve}
\subsection{state why this problem is worth solving}
\subsection{related work}

\section{methodology}
\subsection{reading papers and talking to field experts}
\subsection{list paper 1 and its contribution}
\subsection{list paper 2 and its contribution}
\subsection{list paper 3 and its contribution}
\subsection{list paper 4 and its contribution}
\subsection{list perception engineer and their claims}
\subsection{list robotics engineer and their claims}
\subsection{list professor and their claims}
\subsection{Building the tools}
\subsection{to test claims we needed a standardised enviorment}
\subsection{carla.network()}
\subsection{carla.vehicle}




\section{Experimental Design}
\subsection{Building the tools (Technical details)}
\subsection{to test claims we needed a standardised enviorment(Technical details)}
\subsection{carla.network()(Technical details)}
\subsection{carla.vehicle(Technical details)}

\section{Evaluation}
\subsection{conclusions from the literature}
\subsection{include picture of simulation setup}

\section{future work}
\subsection{standards for:}
\subsection{communication protocal}
\subsection{a minimum robustness based on bandwidth and packet loss rates}
\subsection{a minimum accuricy of detections on XXX dataset(s)}
\subsection{cost of implementation/maintence}
\subsection{emergancy personal protocal}
\subsection{a minimum localisation accuricy on XXX dataset(s)}
\subsection{a minimum distance estimation accurcy}
% standards for
% communication protocal
% a minimum robustness based on bandwidth and packet loss rates
% a minimum accuricy of detections on XXX dataset(s)
% cost of implementation/maintence
% emergancy personal protocal
% a minimum localisation accuricy on XXX dataset(s)
% a minimum distance estimation accurcy

\section{conclusion}
\subsection{tools}
\subsection{need for standarization}
\subsection{processed data is suficient}
% note: call out that the community has decided that things should happen in 100ms but this       needs to be brawdened


\end{document}












% DOCUMENT NOTES FOR TABLES AND FIGURES:
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% ------------------------------------------------------------------------------------
% DOCUMENT NOTES: (Table)
% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}
% ------------------------------------------------------------------------------------

% DOCUMENT NOTES: (Table)
% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}
% ------------------------------------------------------------------------------------

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 









